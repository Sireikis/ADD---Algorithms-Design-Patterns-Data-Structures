<!DOCTYPE html>
<html>
    <head>
        <!-- link to css file-->
        <link rel="stylesheet" href="../css/ContentStyle.css">
            <link rel="stylesheet" href="../css/GreenStyle.css">
                </head>
    <body>
        <h1 class="title">Big O</h1>
        
        <p>
        Big O is a mathematical notation used to represent the upper bound 'speed' of a function, regardless of the hardware used to run the function. Note that this notation is typically considering large input sizes. We will not cover the mathematical aspects of this topic, such as analyzing a function to determine its Big O. However, we will cover the meaning of some common asymptotic notation (O, &Omega;, &Theta;), the functions used to represent them, and the distinction between asymptotic notation and worst/average/best case.
        </p>
        
        <h2>What Problem Does Big O Solve?</h2>
        <p>
        To understand what Big O is it helps to understand what problem it solves. In this day and age there are so many different devices with different levels of computing power at their disposable. Let's say you sit down at your personal computer and you want to know how fast it takes to sort an array of 1000 elements. So you use mergesort and time how fast your computer performs the sort and you get some numerical value, let's say 10 milliseconds. How useful is that information for other people? Is it even useful for your own personal use?
        </p>
        
        <p>
        The problem with measuring the speed of an algorithm in milliseconds (or any other units of time) is that it is completely dependent on the conditions under which the measurement was performed. What hardware was used? What else was that computer doing at that time? What features does the CPU have that may speed up or slow down the ability of the computer to perform the sorting task? What we need is a way to measure the speed of an algorithm in terms that disregard the hardware used to perform the algorithm. This is what Big O does; in computer science asymptotic notation is used to classify algorithms according to how their run time or space requirements grow as the input size grows.
        </p>
        
        <h2>Big Omega, Big Theta, Big O</h2>
        <p>
        Let's consider what a graph that describes the speed of an algorithm in terms that disregard the hardware used to perform the algorithm looks like. We know that the amount of time taken to perform each step of an algorithm will vary between hardware. So let's consider the problem in more general terms, where we call each computation or step taken in an algorithm an <b>operation</b>. For example, an operation could be setting the value of a variable, incrementing a variable, initializing an array, or any such statement. This will be the y axis of our graph, the value that varies from algorithm to algorithm. What do all algorithms share? Well, they all process some input, some discrete number of elements. An example would sorting an array of 10 elements--the size of the input would be 10. This will be the x axis of our graph, the input size of an algorithm.
        </p>
        
        <img src="../Images/BigO Operations VS Inputs.png">
        
        <p>
        Note: Often times certain operations performed in an algorthm will not be significant for the analysis of the algorithm, because of where they are located in the structure of the algorithm. The analysis of algorithms to determine their time complexity will not be covered here.
        </p>
        
        <p>
        Now we can imagine that for some given algorithm, given some input size, we can count the number of operations that the algorithm needs to perform to accomplish its task. However, there's an important detail to consider, which is that the order of the input can often matter. Some sorting algorithms perform badly given an input that's in a specific order. Conversely, some algorithms perform better. To clarify, we are equating a low number of operations to better performance and a high number of operations to worse performance. With this in mind, we often don't get a single line that we can connect our points with, but rather a spread of dots.
        </p>
        
        <p>
        Image here
        </p>
        <!-- Operations vs Input Size, spread plot of example data, match to description below -->
        
        <p>
        Here at input size 10 we see that there's a variance in the number of operations performed when given different variations in input size. So how do we describe the speed of an algorithm given this spread of operations performed for a given input size? The solution is to form lines, generated by some function, that describes the upper and lower limits of operations performed at a given input size. Thus, we can say that a given algorithm will always perform less operations than the line formed by a given function (upper bound) or that it will always perform more operations than the line formed by a given function (lower bound). These bounds are described by the terms below. Examples of common functions that form these lines will be also be given below in the functions section.
        </p>
        
        <p>
        Notice that these bounds can be far above or below the actual number of operations than an algorithm will perform. Thus, it's important to determine the running time of an algorithm as <b>tightly</b> as we can. In other words, forming bounds that are as close to the minimum (lower bound) or maximum (upper bound) number of actual operations performed by an algorithm.
        </p>
        
        <h3>Big O</h3>
        <p>
        Big O refers to the upper bound of an algorithm, which is the maximum number of operations an algorithm will taken given the most unfavorable inputs. What do we mean by unfavorable? Well, let's say we're using a sorting algorithm like quicksort, where the pivot is the rightmost element. If we consider all possible variations in input, the inputs that would take the longest to sort (most operations) would be:
        </p>
        
        <ol>
            <li>Elements are sorted</li>
            <li>Elements are sorted in reverse order</li>
            <li>All elements are the same</li>
        </ol>
        
        <p>
        These are the unfavorable inputs for this version of quicksort--where the pivot is the rightmost element. For an input of any given size <i>n</i>, if the inputs are arranged in the above ways, the quicksort algorithm will perform the most operations to sort those inputs, compared to inputs of the same size arranged in other ways. This sets a concrete ceiling for this algorithm, an upper bound, for how badly it can perform given some input size n.
        </p>
        
        <p>
        I have purposely avoided saying 'worst case' thus far, I will explain the difference between asymptotic notation and the best/average/worst cases below.
        </p>
        
        
        <h3>Big Omega (&Omega;)</h3>
        <p>
        Big &Omega; refers to the lower bound of an algorithm, which is the minimum number of operations an algorithm will taken given the most favorable inputs. What do we mean by favorable? Let's say we're using a sorting algorithm like bubble sort, which will proceed through the given input and check if an element and the next element need to be swapped. If we find no swaps, sorting is considered to be done and the function ends. Under these conditions, if we consider all possible variations in input, the inputs that would take the shortest amount of time to sort (fewest operations) would be:
        </p>
        
        <ol>
            <li>Elements are sorted</li>
            <li>All elements are the same</li>
        </ol>
        
        <p>
        These are the favorable inputs for this version of bubble sort. For an input of any given size <i>n</i>, if the inputs are arranged in the above ways, the bubble sort algorithm will perform the fewest operations to sort those inputs, compared to inputs of the same size arranged in other ways. This sets a concrete floor for this algorithm, a lower bound, for how well it can perform given some input size n.
        </p>
        
        <p>
        I have purposely avoided saying 'best case' thus far, I will explain the difference between asymptotic notation and the best/average/worst cases below.
        </p>
        
        <h3>Big Theta (&Theta;)</h3>
        <p>
        Big &Theta; refers to the lower and upper bounds of an algorithm, which are the maximum and minimum number of operations that an algorithm performs under some set of conditions. You can imagine it as being a pair of lines which contain between them the number of operations an algorithm performs under some set of conditions. The pair of lines are formed by the same function, however the constants that multiple the value of the function differ between the lower and upper bounds.
        </p>
        
        <p>
        Image here
        </p>
        <!-- Big Theta tightly bound graph -->
        
        <p>
        Once the input size is sufficiently large, the number of operations will fall between <var>k<sub>1</sub></var> &#x2219; <var>f(n)</var> and <var>k<sub>2</sub></var> &#x2219; <var>f(n)</var>.
        </p>
        
        <p>
        Notice that we make a reference to the lower and upper bounds of an algorithm. You'd be right to think that these refer to Big &Omega; and Big O. In fact, Big &Theta; can be used to determine a tight bound on the number of operations for an algorithm only when the Big O and Big &Omega; are the same for some set of conditions. More formally, <b>Big &Theta;</b> is the intersection of the sets of functions that determine the lower bound (Big &Omega;) and the upper bound (Big O). Remember that Big O can overshoot and Big &Omega; can undershoot (they can be loosely bound), so all functions that contain the number of operations for the algorithm in question are valid functions for representing Big O or Big &Omega;. Hence, if there is a function that is shared between both groups of functions (the intersection), then it is the function that most tightly binds the operations of an algorithm. If there is no such function, then there is no Big &Theta; for the algorithm in question. In such a case, at best we can describe its operations using the bounds given by Big O and Big &Omega;. It's worthwhile to note that of the two, Big O binds an algorithm more tightly. Since Big &Omega; sets a floor, there is an infinite celing above it. Meanwhile, Big O sets a ceiling, which means we will eventually reach the floor.
        </p>
        
        <p>
        I have purposely avoided saying 'average case' thus far, I will explain the difference between asymptotic notation and the best/average/worst cases below.
        </p>
        
        <h2>Common Functions</h2>
        <p>
        It's finally time to discuss what the functions used in asymptotic notation look like. Remember our Operations vs Input Size graph? Well we can write relatively simple functions that, given a value on our x-axis (Input Size), will return some value on the y-axis (Operations). If we repeat this for all x values (all Input Sizes), we will eventually form a line. These lines are what form the bounds we've been referring to! The only distinction between saying that some line is representing Big &Omega; or Big O, is that we've decided that this line will be a floor or a ceiling. In other words, we can call any function the Big &Omega; or Big O, the functions themselves aren't tied to any asymptotic notation. Rather, the notation is being used to describe the relationship the function will have to the algorithm's Operations vs Input Size graph. Let's look at some of these functions.
        </p>
        
        <table>
            <tr>
                <th>Function</th>
                <th>Name</th>
                <th>Description</th>
            </tr>
            
            <tr>
                <td><b>(1)</b></td>
                <td>Constant</td>
                <td>
                    This forms a line where, regardless of the input size, the number of operations will remain constant. This represents algorithms whose number of operations don't change as the number of inputs increases. An example is access time in an array, no matter how many elements the array contains, access is performed in one operation.
                </td>
            </tr>
            
            <tr>
                <td><b>(log n)</b></td>
                <td>Logarithmic</td>
                <td>
                    This line represents algorithms where each operation halves the number of inputs being processed. An example would be binary search, where each operation the algorithm performs serves to halve the number of inputs we search through.
                </td>
            </tr>
            
            <tr>
                <td><b>(n)</b></td>
                <td>Linear</td>
                <td>
                    This line represents algorithms where processing each input requires a single operation. For example, performing linear search on an array with 100 elements, where the element we are looking for does not exist, will require 100 operations. We will need to look through every element in the array.
                </td>
            </tr>
            
            <tr>
                <td><b>(nlogn)</b></td>
                <td>Linearithmic</td>
                <td>
                    This describes an algorithm that repeatedly reduces the number of inputs to one via halving, which occurs a number of times equal to the input size. In other words, for each input, we perform the number of operations required to reduce the input size to a single input, via the method of repeatedly halving the number of inputs. An example would be mergesort, which divides the entire input in half, then subdivides those inputs by half, and repeats this process until one input remains, which occurs for all inputs.
                </td>
            </tr>
            
            <tr>
                <td><b>(n<sup>2</sup>)</b></td>
                <td>Quadratic</td>
                <td>
                    This line represents algorithms where for each input we perform a number of operations equal to the input size. This occurs in algorithms that have nested loops that both iterate over the entire input.
                </td>
            </tr>
            
            <tr>
                <td><b>(2<sup>n</sup>)</b></td>
                <td>Exponential</td>
                <td>
                    This line represents algorithms where each additional input doubles the number of operations that are performed. An example is finding all subsets of a set, where finding a subset is a single operation. Each time we increase the number of inputs by one in a set, the number of subsets doubles.
                </td>
            </tr>
            
            <tr>
                <td><b>(n!)</b></td>
                <td>Factorial</td>
                <td>
                    This describes algorithms where each additional input performs the number of operations performed by the last input multiplied by the size of the current input. An example would be finding all permutations of a string. If a string is 4 letters long the number of ways we can rearrange those letters is 24. A 5 letter string can be rearranged in 120 different ways (5*24).
                </td>
            </tr>
        </table>

        <p>
        Below are these functions plotted on our Operations Vs Input Size graph. Our graph legend shows these function as depicting Big O bounds, but they could also be Big &Omega; or Big &Theta;!
        </p>
        
        <img src="../Images/BigO Complexities.png">
        
        <h2>Best, Worst, Average Case</h2>
        <p>
        Thus far when describing Big O or Big &Omega;, I have used the types of inputs that cleary depict the ceiling or floor of an algorithm (upper and lower bounds). However, it's important to realize that there is a separation between the types of input and the asymptotic notation we use to classify them. Each type of asymptotic notation can be applied to any type of input, whether that be input that an algorithm operates favorably or unfavorably on.
        </p>
        
        <p>
        Let's consider quicksort again, where the pivot is the rightmost element. If we consider all possible variations in input, we know the inputs that the algorithm would take the most operations to sort would be:
        </p>
        
        <ol>
            <li>Elements are sorted</li>
            <li>Elements are sorted in reverse order</li>
            <li>All elements are the same</li>
        </ol>
        
        <p>
        Hence, considering this <b>worst</b> case scenario, we could describe the number of operations quicksort performs to be:
        </p>
        
        <ol>
            <li>At most O(n<sup>2</sup>)</li>
            <li>At least &Omega;(n)</li>
        </ol>
        
        <p>
        For this worst case scenario, we describe the number of operations performed by quicksort to be at most O(n<sup>2</sup>) or at least &Omega;(n). However, Big O binds more tightly, so we would prefer to use Big O to describe this worst case scenario.
        </p>
        
        <p>
        The same can be said for the other 'cases', such as the best case, where the inputs are arranged in an ideal manner for any given input size. We can describe these cases using any combination of asymptotic notation, but we typically use one that binds the algorithm as tightly as possible (Big &Theta; idealy), because it describes an algorithm precisely.
        </p>
     
        <p>
        To recap, the best, average, and worst cases describe various classes of inputs, whether they be favorable, unfavorable, or in between. The number of operations an aglorithm will perform under these circumstances can be described in terms of asymptotic notation--whether that be upper or lower bounds or both.
        </p>
        
        <h2>A Final Note</h2>
        <p>
        Thus far we've been using Operations as an easy to understand measurement. However, in the actual analysis of algorithms we measure the time or space complexity. <b>Complexity</b> is a relative measure to something else. So far our complexity has been the number of operations relative to the input size. We've been describing the growth rate for operations performed in an algorithm as a function of n (input size). We can express the growth rate of time or space complexity in the same way, what changes is the label of the y-axis--we are still comparing the growth rate to the number of inputs. If f(x) refers to the growth rate of some aspect of an algorithm, given large enough inputs, we can generalize this to:
        </p>
        
        <ul>
            <li>If f(x) is below O(<i>upperbound</i>), then f 'grows no faster than' <i>upperbound</i></li>
            <li>If f(x) is contained within &Theta;(<i>tightbound</i>), then f 'grows exactly like' <i>tightbound</i></li>
            <li>If f(x) is above &Omega;(<i>lowerbound</i>), then f 'grows no slower than' <i>upperbound</i></li>
        </ul>
        
        <p>
        We didn't cover how to analyse an algorithm to reduce it's complexity to the functions we described above. What I will say is that asymptotic notation is all about how things scale. Given sufficiently large inputs, when we analyse a function we can ignore everything except for the largest term. This includes any constant factors. If for example we calculated our operations in terms of n to be 6n<sup>2</sup> + 10n + 32, our complexity would be described by O(n<sup>2</sup>).
        </p
    </body>
</html>


